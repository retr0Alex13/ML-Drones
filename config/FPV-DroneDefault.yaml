behaviors:
  FPV-Drone:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128   # Typical range: (Discrete, PPO & SAC): 32 - 512. This should always be multiple times smaller than buffer_size. If you are using only discrete actions, this value should be smaller (on the order of 10s).
      buffer_size: 2048     # default = 10240 for PPO. Typical range: PPO: 2048 - 409600; This should be multiple times larger than batch_size. Typically a larger buffer_size corresponds to more stable training updates.
      learning_rate: 0.0003     # (default = 0.0003) Typical range: 0.00001 - 0.001. This should typically be decreased if training is unstable, and the reward does not consistently increase.
      beta: 0.005    # (default = 0.005) Typical range: 0.0001 - 0.01. If entropy drops too quickly, increase beta. If entropy drops too slowly, decrease beta.
      epsilon: 0.2  # (default = 0.2) Typical range: 0.1 - 0.3. Setting this value small will result in more stable updates, but will also slow the training process.
      lambd: 0.95   # (default = 0.95) Typical range: 0.9 - 0.95. Low values correspond to relying more on the current value estimate (which can be high bias), and high values correspond to relying more on the actual rewards received in the environment (which can be high variance).
      num_epoch: 3  # (default = 3) Typical range: 3 - 10. The larger the batch_size, the larger it is acceptable to make this. Decreasing this will ensure more stable updates, at the cost of slower learning.
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 512     # (default = 128) Typical range: 32 - 1024. For simple problems where the correct action is a straightforward combination of the observation inputs, this should be small. For problems where the action is a very complex interaction between the observation variables, this should be larger.
      num_layers: 2     # (default = 2) Typical range: 1 - 3.  For simple problems, fewer layers are likely to train faster and more efficiently. More layers may be necessary for more complex control problems.
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.99     # (default = 0.99) Typical range: 0.8 - 0.995. 0.8 - 0.995 In situations when the agent should be acting in the present in order to prepare for rewards in the distant future, this value should be large. In cases when rewards are more immediate, it can be smaller. Must be strictly smaller than 1.
        strength: 1.0
    keep_checkpoints: 5
    max_steps: 10000000
    time_horizon: 256   # (default = 64) Typical range: 32 - 2048. This number should be large enough to capture all the important behavior within a sequence of an agent's actions.
    summary_freq: 30000
